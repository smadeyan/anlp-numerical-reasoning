{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import argparse\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import torch\n",
    "from statistics import mean\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from utils import create_demo_text, MyDataset, setup_data_loader, parse_arguments\n",
    "from decoder import Decoder, answer_cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raj/.local/lib/python3.7/site-packages/datasets/load.py:1763: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=no_checks' instead.\n",
      "  FutureWarning,\n",
      "Found cached dataset parquet (/home/raj/.cache/huggingface/datasets/parquet/main-a198226013e23936/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|██████████| 2/2 [00:00<00:00, 804.82it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"gsm8k\", ignore_verifications=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = dataset['train']\n",
    "data_test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reader(args=None, data=None):\n",
    "\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    for datum in data:\n",
    "        questions.append(datum[\"question\"].strip())\n",
    "        answers.append(datum[\"answer\"].split(\"#### \")[-1]) \n",
    "    \n",
    "    q_len_list = []\n",
    "    for q in questions:\n",
    "        q_len_list.append(len(q.split(\" \")))\n",
    "    q_len_mean = mean(q_len_list)\n",
    "    \n",
    "    print(\"dataset : {}\".format('gsm8k'))\n",
    "    print(\"data size : {}\".format(len(answers)))\n",
    "    print(\"average num of words for each sample : {}\".format(q_len_mean))\n",
    "    \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \n",
    "    print(\"OPENAI_API_KEY:\")\n",
    "    print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    # Initialize decoder class (load model and tokenizer) ...\n",
    "    decoder = Decoder(args=args)\n",
    "    \n",
    "    print(\"setup data loader ...\")\n",
    "    data_ = data_test\n",
    "\n",
    "    demo = create_demo_text(args, cot_flag=True)\n",
    "    \n",
    "    total = 0\n",
    "    correct_list = []\n",
    "            \n",
    "    for i, data in enumerate(data_):\n",
    "        print('*************************')\n",
    "        print(\"{}st data\".format(i+1))\n",
    "                \n",
    "        # Prepare question template ...\n",
    "        x, y = data\n",
    "        x = \"Q: \" + x[0] + \"\\n\" + \"A:\"\n",
    "        y = y[0].strip()\n",
    "        \n",
    "        if args.method == \"zero_shot\":\n",
    "            x = x + \" \" + args.direct_answer_trigger_for_zeroshot\n",
    "        elif args.method == \"zero_shot_cot\":\n",
    "            x = x + \" \" + args.cot_trigger\n",
    "        elif args.method == \"few_shot\":\n",
    "            x = demo + x\n",
    "        elif args.method == \"few_shot_cot\":\n",
    "            x = demo + x\n",
    "        else:\n",
    "            raise ValueError(\"method is not properly defined ...\")\n",
    "        \n",
    "        # Answer prediction by generating text ...\n",
    "        max_length = 128 if \"cot\" in args.method else 32\n",
    "        z = decoder.decode(args, x, max_length, i, 1)\n",
    "\n",
    "        # Answer extraction for zero-shot-cot ...\n",
    "        if args.method == \"zero_shot_cot\":\n",
    "            z2 = x + z + \" \" + args.direct_answer_trigger_for_zeroshot_cot\n",
    "            max_length = args.max_length_direct\n",
    "            pred = decoder.decode(args, z2, max_length, i, 2)\n",
    "            print(z2 + pred)\n",
    "        else:\n",
    "            pred = z\n",
    "            print(x + pred)\n",
    "\n",
    "        # Clensing of predicted answer ...\n",
    "        pred = answer_cleansing(args, pred)\n",
    "        \n",
    "        # Choose the most frequent answer from the list ...\n",
    "        print(\"pred : {}\".format(pred))\n",
    "        print(\"GT : \" + y)\n",
    "        print('*************************')\n",
    "        \n",
    "        # Checking answer ...\n",
    "        correct = (np.array([pred]) == np.array([y])).sum().item()\n",
    "        correct_list.append(correct)\n",
    "        total += 1 #np.array([y]).size(0)\n",
    "        \n",
    "        if i%5 == 0:\n",
    "            break\n",
    "    \n",
    "    # Calculate accuracy ...\n",
    "    accuracy = (sum(correct_list) * 1.0 / total) * 100\n",
    "    print(\"accuracy : {}\".format(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--api_log_file_name API_LOG_FILE_NAME]\n",
      "                             [--random_seed RANDOM_SEED]\n",
      "                             [--dataset {aqua,gsm8k,commonsensqa,addsub,multiarith,strategyqa,svamp,singleeq,bigbench_date,object_tracking,coin_flip,last_letters}]\n",
      "                             [--minibatch_size {1}]\n",
      "                             [--max_num_worker MAX_NUM_WORKER]\n",
      "                             [--model {gpt3,gpt3-medium,gpt3-large,gpt3-xl}]\n",
      "                             [--method {zero_shot,zero_shot_cot,few_shot,few_shot_cot}]\n",
      "                             [--cot_trigger_no COT_TRIGGER_NO]\n",
      "                             [--max_length_cot MAX_LENGTH_COT]\n",
      "                             [--max_length_direct MAX_LENGTH_DIRECT]\n",
      "                             [--limit_dataset_size LIMIT_DATASET_SIZE]\n",
      "                             [--api_time_interval API_TIME_INTERVAL]\n",
      "                             [--log_dir LOG_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/raj/.local/share/jupyter/runtime/kernel-v2-1163rzSarXflQeAd.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raj/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "args = parse_arguments()\n",
    "main(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-UrWEfDNtKOBV9AZ-hLjA-Q https://cmu.litellm.ai\n",
      "ChatCompletion(id='chatcmpl-9Fc59smDt6mwzsB8Ta31Vzt5wdhuc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"In a world of chaos and strife,\\nA moment of calm in the stormy life.\\nSoft whispers of the wind,\\nBringing peace to the troubled mind.\\nNature's beauty all around,\\nA gentle solace that can be found.\\nIn this fleeting moment of grace,\\nWe find our hearts in a tranquil place.\", role='assistant', function_call=None, tool_calls=None))], created=1713508335, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_d9767fc5b9', usage=CompletionUsage(completion_tokens=63, prompt_tokens=17, total_tokens=80))\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"PROXY_API_KEY\")\n",
    "base_url = os.getenv(\"PROXY_BASE_URL\")\n",
    "\n",
    "print(api_key, base_url)\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # model to use from Models Tab\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"this is a test request, write a short poem\"}\n",
    "    ],\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lOAD LLAMA2-7b model from huggingface\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
