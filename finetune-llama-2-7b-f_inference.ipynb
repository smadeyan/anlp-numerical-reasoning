{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2c08747",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart - invoke text generation endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aeadc7",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to attach a predictor to an existing endpoint name and invoke the endpoint with example payloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a4c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import retrieve_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6101c895",
   "metadata": {},
   "source": [
    "Retrieve a predictor from your deployed endpoint name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "badd73a3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoint_name = \"gsm8k-encs1-meta-textgeneration-l-20240420-203637\"\n",
    "endpoint_name = \"jumpstart-dft-meta-textgeneration-l-20240427-203329\"\n",
    "predictor = retrieve_default(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc96b5",
   "metadata": {},
   "source": [
    "Now query your endpoint with example payloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2306488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Mayonnaise is a thick, creamy condiment made from a mixture of egg yolks, oil, vinegar or lemon juice, and seasonings. Here is a basic recipe for homemade mayonnaise:\\n\\nIngredients:\\n\\n* 2 egg yolks\\n* 1/2 cup (120 ml) neutral-tasting oil, such as canola or grapeseed\\n* 1 tablespoon (15 ml) vinegar or lemon juice\\n* Salt and pepper to taste\\n\\nInstructions:\\n\\n1. In a small bowl, whisk together the egg yolks and vinegar or lemon juice until smooth and slightly thickened.\\n2. Slowly pour the oil into the egg yolk mixture while continuously whisking.\\n3. Continue whisking until the mixture thickens and emulsifies, forming a smooth and creamy texture.\\n4. Season with salt and pepper to taste.\\n5. Cover and refrigerate for at least 30 minutes before serving.\\n\\nNote: You can also add other ingredients'}]\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"<s>[INST] what is the recipe of mayonnaise? explain in a few lines [/INST] \",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "response = predictor.predict(payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e650e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '\\n\\n### What is so great about #1?\\n\\n> **What is so great about #1?**\\n\\n**What is so great about #1?**\\n\\n> **What is so great about #1?**\\n\\n**What is so great about #1?**\\n\\n> **What is so great about #1?**\\n\\n**What is so great about #1?**\\n\\n> **What is so great about #1?**\\n\\n**What is so great about #1?**\\n\\n> **What is so great about #1?**\\n\\n**What is so great about #1?**\\n\\n> **What is so great about #1?**\\n\\n**What is so great about #1?**\\n\\n> **What is so great about #1?**\\n\\n**What is so great about #1?**\\n\\n> **What is so great about #1?**\\n\\n**What is so great about #1?**\\n\\n> **What is so great about #1?**\\n\\n**What is so great about #1?**\\n\\n> **What is so great about #1?**\\n\\n**What is so'}]\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"<s>[INST] I am going to Paris, what should I see? [/INST] Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.</s><s>[INST] What is so great about #1? [/INST] \",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "response = predictor.predict(payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c99141ab-985a-47b7-ac41-4c6b9e80059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"The Eiffel Tower is considered one of the most iconic and recognizable landmarks in the world, and there are several reasons why it's so great:\\n\\n1. Unique Design: The Eiffel Tower's design is unlike any other structure in the world. It was created by Gustave Eiffel and his engineering company for the 1889 World's Fair in Paris, and its lattice-like framework was revolutionary for its time. The tower's unique design makes it a standout among other landmarks and a symbol of Paris and France.\\n2. Historical Significance: The Eiffel Tower was built for the World's Fair, which was held in Paris to celebrate the 100th anniversary of the French Revolution. It was originally intended to be a temporary structure, but it became an instant icon of the city and a symbol of French culture and engineering.\\n3. Breathtaking Views: The Eiffel Tower offers panoramic views of Paris from its observation decks, which are located on the first and second levels. Visitors can see many of the city's famous landmarks, including the Seine River, the Arc de Triomphe, and the Montmartre neighborhood.\\n4. Cultural Significance: The Eiffel Tower has been featured in countless films, books, and artworks, and has become a symbol of romance and love. It's often seen as a symbol of Paris and France, and is one of the most recognizable landmarks in the world.\\n5. Engineering Marvel: The Eiffel Tower is an engineering marvel, with a lattice-like framework that was revolutionary for its time. It was built using over 18,000 pieces of wrought iron and weighs over 10,000 tons.\\n6. Accessibility: The Eiffel Tower is easily accessible by public transportation, with several metro stations nearby and a convenient shuttle service.\\n7. Nighttime Views: The Eiffel Tower is particularly beautiful at night, when it's lit up with thousands of twinkling lights. Visitors can enjoy the view from the observation decks or from a distance, as the tower is visible from many parts of the city.\\n\\nOverall, the Eiffel Tower is a must-see attraction in Paris, offering a unique and unfor\"}]\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"<s>[INST] I am going to Paris, what should I see? [/INST] Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.</s><s>[INST] What is so great about #1? [/INST] \",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "response = predictor.predict(payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d09cd374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Great, let\\'s get started! The probability of rain tomorrow is given as $frac{3}{10}$. To find the probability that it will not rain tomorrow, we can use the concept of complementary probability.\\n\\nThe complementary probability of an event A is the probability of the event A\\' (the complement of A), which is the probability of all outcomes that are not in A. In this case, the event \"it will not rain tomorrow\" is the complement of the event \"it will rain tomorrow\".\\n\\nSo, the probability that it will not rain tomorrow is:\\n\\nComplement of $frac{3}{10} = frac{7}{10}$\\n\\nTherefore, the probability that it will not rain tomorrow is $frac{7}{10}$.'}]\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"<s>[INST] <<SYS>>\\nYou are a diligent math student, logically solving given math problems\\n<</SYS>>\\n\\nThe probability of rain tomorrow is $\\frac{3}{10}$. What is the probability that it will not rain tomorrow? Express your answer as a common fraction. [/INST] \",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "response = predictor.predict(payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533b1cd-0cc6-4874-bd36-153900e3efb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd40a234-fe4d-4da8-b22c-ea438896f324",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (422) from 9deX2bJkth7Kltwjhtm8cdKIBAv0Im0xsOGd with message \"Failed to deserialize the JSON body into the target type: inputs: invalid type: sequence, expected a string at line 1 column 11\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/jumpstart-dft-meta-textgeneration-l-20240427-203329 in account 471112774062 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 20\u001b[0m\n\u001b[1;32m      1\u001b[0m example \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExample 1\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSentence: Though the sun set with a brilliant display of colors, casting a warm glow over the serene beach, it was the bitter news I received earlier that clouded my emotions, making it impossible to truly appreciate nature\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms beauty.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFollowing the same format above from the examples, What is the sentiment of this setence: While the grandeur of the ancient castle, steeped in history and surrounded by verdant landscapes, was undeniably breathtaking, the knowledge that it was the site of numerous tragic events lent an undeniable heaviness to its majestic walls.\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     12\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [[\n\u001b[1;32m     14\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a customer agent\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m512\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.6\u001b[39m}\n\u001b[1;32m     18\u001b[0m }\n\u001b[0;32m---> 20\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/base_predictor.py:209\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes, component_name)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inference_component_name:\n\u001b[1;32m    207\u001b[0m     request_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInferenceComponentName\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m inference_component_name\n\u001b[0;32m--> 209\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_runtime_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (422) from 9deX2bJkth7Kltwjhtm8cdKIBAv0Im0xsOGd with message \"Failed to deserialize the JSON body into the target type: inputs: invalid type: sequence, expected a string at line 1 column 11\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/jumpstart-dft-meta-textgeneration-l-20240427-203329 in account 471112774062 for more information."
     ]
    }
   ],
   "source": [
    "\n",
    "example = \"\"\"\n",
    "            \\n\\nExample 1\n",
    "            \\nSentence: Though the sun set with a brilliant display of colors, casting a warm glow over the serene beach, it was the bitter news I received earlier that clouded my emotions, making it impossible to truly appreciate nature's beauty.\n",
    "            \\nSentiment: Negative\n",
    "                                    \n",
    "            \\n\\nExample 2\n",
    "            \\nSentence: Even amidst the pressing challenges of the bustling city, the spontaneous act of kindness from a stranger, in the form of a returned lost wallet, renewed my faith in the inherent goodness of humanity.\n",
    "            \\nSentiment: Positive\n",
    "                                    \n",
    "            \\n\\nFollowing the same format above from the examples, What is the sentiment of this setence: While the grandeur of the ancient castle, steeped in history and surrounded by verdant landscapes, was undeniably breathtaking, the knowledge that it was the site of numerous tragic events lent an undeniable heaviness to its majestic walls.\n",
    "            \"\"\"\n",
    "payload = {\n",
    "    \"inputs\": \"<s>[INST] <<SYS>>\\nYou are an excellent math student solving the given math problem\\n<</SYS>>\\n\\nTo express 20 as a sum of different powers of 2, we would write $20 = 2^4 + 2^2$. The sum of the exponents of these powers is $4 + 2 = 6$. If 400 were expressed as a sum of at least two distinct powers of 2, what would be the least possible sum of the exponents of these powers? [/INST] \",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408e338-32f0-4491-adf4-a169ba52b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"<s>[INST] <<SYS>>\\nExplain your reasoning\\n<</SYS>>\\n\\nForty percent of the students have elected to learn from home during the pandemic. The remaining students are divided into two equal groups, only one of which is physically in school on any day. What percent of students are present in school? [/INST] \",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "\n",
    "def remove_tags(text):\n",
    "    # Define the regex pattern to match <se>NUMBER<de>NUMBER<fe> and similar tags\n",
    "    # This pattern matches the tags and their entire contents\n",
    "    pattern = r\"<se>\"\n",
    "    \n",
    "    # Use re.sub() to replace all occurrences of the pattern with an empty string\n",
    "    cleaned_text = re.sub(pattern, \"\", text)\n",
    "\n",
    "    pattern = r\"<de>\"\n",
    "    cleaned_text = re.sub(pattern, \"\", cleaned_text)\n",
    "\n",
    "    pattern = r\"<fe>\"\n",
    "    cleaned_text = re.sub(pattern, \"\", cleaned_text)\n",
    "    \n",
    "    # Additionally, if there are nested tags like <<<...>>>, handle these\n",
    "    # This pattern will match any number of sequential < or > characters\n",
    "    # cleaned_text = re.sub(r\"<+|>+\",\"\", cleaned_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "response = predictor.predict(payload)\n",
    "print(response)\n",
    "print(remove_tags(response[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4bf5fc-70d7-4d6c-aae4-64ca3225b1aa",
   "metadata": {},
   "source": [
    "# GSM8K 8-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16bdd3-9d7c-4954-a559-e8e15bc4355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, z, y = [], [], []\n",
    "x.append(\"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\")\n",
    "z.append(\"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6.\")\n",
    "y.append(\"6\")\n",
    "\n",
    "x.append(\"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\")\n",
    "z.append(\"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5.\")\n",
    "y.append(\"5\")        \n",
    "\n",
    "x.append(\"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\")\n",
    "z.append(\"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39.\")\n",
    "y.append(\"39\")        \n",
    "\n",
    "x.append(\"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\")\n",
    "z.append(\"Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8.\")\n",
    "y.append(\"8\")        \n",
    "\n",
    "x.append(\"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\")\n",
    "z.append(\"Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9.\")\n",
    "y.append(\"9\")        \n",
    "\n",
    "x.append(\"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\")\n",
    "z.append(\"There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29.\")\n",
    "y.append(\"29\")        \n",
    "\n",
    "x.append(\"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\")\n",
    "z.append(\"Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls.\")\n",
    "y.append(\"33\")        \n",
    "\n",
    "x.append(\"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\")\n",
    "z.append(\"Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8.\")\n",
    "y.append(\"8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2d531b35-9d46-4bb4-94fb-bf5fb471eec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 lines...\n",
      "Processed 20 lines...\n",
      "Processed 30 lines...\n",
      "Processed 40 lines...\n",
      "Processed 50 lines...\n",
      "Processed 60 lines...\n",
      "Processed 70 lines...\n",
      "Processed 80 lines...\n",
      "Processed 90 lines...\n",
      "Processed 100 lines...\n",
      "Processed 110 lines...\n",
      "Processed 120 lines...\n",
      "Processed 130 lines...\n",
      "Processed 140 lines...\n",
      "Processed 150 lines...\n",
      "Processed 160 lines...\n",
      "Processed 170 lines...\n",
      "Processed 180 lines...\n",
      "Processed 190 lines...\n",
      "Processed 200 lines...\n",
      "Processed 210 lines...\n",
      "Processed 220 lines...\n",
      "Processed 230 lines...\n",
      "Processed 240 lines...\n",
      "Processed 250 lines...\n",
      "Processed 260 lines...\n",
      "Processed 270 lines...\n",
      "Processed 280 lines...\n",
      "Processed 290 lines...\n",
      "Processed 300 lines...\n",
      "Processed 310 lines...\n",
      "Processed 320 lines...\n",
      "Processed 330 lines...\n",
      "Processed 340 lines...\n",
      "Processed 350 lines...\n",
      "Processed 360 lines...\n",
      "Processed 370 lines...\n",
      "Processed 380 lines...\n",
      "Processed 390 lines...\n",
      "Processed 400 lines...\n",
      "Processed 410 lines...\n",
      "Processed 420 lines...\n",
      "Processed 430 lines...\n",
      "Processed 440 lines...\n",
      "Processed 450 lines...\n",
      "Processed 460 lines...\n",
      "Processed 470 lines...\n",
      "Processed 480 lines...\n",
      "Processed 490 lines...\n",
      "Processed 500 lines...\n",
      "Processed 510 lines...\n",
      "Processed 520 lines...\n",
      "Processed 530 lines...\n",
      "Processed 540 lines...\n",
      "Processed 550 lines...\n",
      "Processed 560 lines...\n",
      "Processed 570 lines...\n",
      "Processed 580 lines...\n",
      "Processed 590 lines...\n",
      "Processed 600 lines...\n",
      "Processed 610 lines...\n",
      "Processed 620 lines...\n",
      "Processed 630 lines...\n",
      "Processed 640 lines...\n",
      "Processed 650 lines...\n",
      "Processed 660 lines...\n",
      "Processed 670 lines...\n",
      "Processed 680 lines...\n",
      "Processed 690 lines...\n",
      "Processed 700 lines...\n",
      "Processed 710 lines...\n",
      "Processed 720 lines...\n",
      "Processed 730 lines...\n",
      "Processed 740 lines...\n",
      "Processed 750 lines...\n",
      "Processed 760 lines...\n",
      "Processed 770 lines...\n",
      "Processed 780 lines...\n",
      "Processed 790 lines...\n",
      "Processed 800 lines...\n",
      "Processed 810 lines...\n",
      "Processed 820 lines...\n",
      "Processed 830 lines...\n",
      "Processed 840 lines...\n",
      "Processed 850 lines...\n",
      "Processed 860 lines...\n",
      "Processed 870 lines...\n",
      "Processed 880 lines...\n",
      "Processed 890 lines...\n",
      "Processed 900 lines...\n",
      "Processed 910 lines...\n",
      "Processed 920 lines...\n",
      "Processed 930 lines...\n",
      "Processed 940 lines...\n",
      "Processed 950 lines...\n",
      "Processed 960 lines...\n",
      "Processed 970 lines...\n",
      "Processed 980 lines...\n",
      "Processed 990 lines...\n",
      "Processed 1000 lines...\n",
      "Processed 1010 lines...\n",
      "Processed 1020 lines...\n",
      "Processed 1030 lines...\n",
      "Processed 1040 lines...\n",
      "Processed 1050 lines...\n",
      "Processed 1060 lines...\n",
      "Processed 1070 lines...\n",
      "Processed 1080 lines...\n",
      "Processed 1090 lines...\n",
      "Processed 1100 lines...\n",
      "Processed 1110 lines...\n",
      "Processed 1120 lines...\n",
      "Processed 1130 lines...\n",
      "Processed 1140 lines...\n",
      "Processed 1150 lines...\n",
      "Processed 1160 lines...\n",
      "Processed 1170 lines...\n",
      "Processed 1180 lines...\n",
      "Processed 1190 lines...\n",
      "Processed 1200 lines...\n",
      "Processed 1210 lines...\n",
      "Processed 1220 lines...\n",
      "Processed 1230 lines...\n",
      "Processed 1240 lines...\n",
      "Processed 1250 lines...\n",
      "Processed 1260 lines...\n",
      "Processed 1270 lines...\n",
      "Processed 1280 lines...\n",
      "Processed 1290 lines...\n",
      "Processed 1300 lines...\n",
      "Processed 1310 lines...\n",
      "0.2805155420773313\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"<s>[INST] <<SYS>>\\nExplain your reasoning\\n<</SYS>>\\n\\nForty percent of the students have elected to learn from home during the pandemic. The remaining students are divided into two equal groups, only one of which is physically in school on any day. What percent of students are present in school? [/INST] \",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "\n",
    "#  print(payload)\n",
    "\n",
    "ground_truths = []\n",
    "inference_answers = []\n",
    "\n",
    "def compute_accuracy(pred: list, gold: list):\n",
    "    acc = 0.0\n",
    "    for p, g in zip(pred, gold):\n",
    "        if p == g:\n",
    "            acc += 1\n",
    "\n",
    "    return acc / len(pred)\n",
    "\n",
    "def generate_complete_user_prompt(question):\n",
    "    prompt_template = \"<s>[INST] <<SYS>>\\\\nExplain your reasoning\\\\n<</SYS>>\\\\n\\\\n{} [/INST] \".format(question)\n",
    "    \n",
    "    return prompt_template\n",
    "\n",
    "def format_few_shot_prompt(system_prompt, questions, answers, final_ans, actual_question):\n",
    "    few_shot_prompt = \"\"\n",
    "    # \"inputs\": \"<s>[INST] <<SYS>>\\nYou are a diligent math student, logically solving given math problems\\n<</SYS>>\\n\\nIf you double a number and add 5 to the result, then that's 20 more than half of the original number. What's the original number? [/INST] Let x be the original number.\\n2*x+5=20+x/2\\n2*x-x/2=15\\n4*x-x=30\\n3*x=30\\nx=<<10=10>>10\\n#### 4</s><s>[INST] What is the units digit of the product of all the odd positive integers between 10 and 110? [/INST] \"\n",
    "    for i in range(len(questions)):\n",
    "        if i == 0:\n",
    "            few_shot_prompt += \"\\n\\n\" + questions[i] + \"\\n[/INST] \\n\" + answers[i] + \"\\n</s>\"\n",
    "        else:\n",
    "            few_shot_prompt += \"\\n<s>\\n[INST] \\n\" + questions[i] + \"\\n[/INST] \\n\" + answers[i] + \"\\n</s>\"\n",
    "\n",
    "    few_shot_prompt += \"<s>\\n[INST] \\n\" + actual_question + \" \\n[/INST] \"\n",
    "    few_shot_prompt = \"<s>\\n[INST] \\n<<SYS>> \\n\" + system_prompt + \" \\n<</SYS>>\" + few_shot_prompt\n",
    "    # print(few_shot_prompt)\n",
    "    return few_shot_prompt\n",
    "\n",
    "def get_ground_truth_numerical_answer(answer):\n",
    "    ans = answer.split('####')[-1]\n",
    "    ans = ans.replace(',', '')  # handle numbers like 2,000\n",
    "    try:\n",
    "        ans = float(ans)\n",
    "    except ValueError:\n",
    "        ans = float(\"inf\")\n",
    "    return ans\n",
    "\n",
    "def extract_answer_number(sentence: str) -> float:\n",
    "    sentence = sentence.replace(',', '')\n",
    "    pred = [s for s in re.findall(r'-?\\d+\\.?\\d*', sentence)]\n",
    "    if not pred:\n",
    "        return float('inf')\n",
    "    segment = sentence.split(\"The final answer is \")\n",
    "    if len(segment) > 1:\n",
    "        pred_answer = segment[1]\n",
    "        pred_answer = [s for s in re.findall(r'-?\\d+\\.?\\d*', pred_answer)]\n",
    "        if len(pred_answer) > 0:\n",
    "            pred_answer = pred_answer[0]\n",
    "        else:\n",
    "            pred_answer = float(pred[-1])\n",
    "    else:\n",
    "        # use the last number as the answer\n",
    "        pred_answer = float(pred[-1])\n",
    "\n",
    "    if isinstance(pred_answer, str):\n",
    "        try:\n",
    "            pred_answer = float(pred_answer)\n",
    "        except ValueError as e:\n",
    "            pred_answer = float('inf')\n",
    "    return pred_answer\n",
    "\n",
    "def perform_gsm8k_inference(input_filename, output_filename):\n",
    "\n",
    "    # Uncomment and run this if you want to see how the text looks before and after encoding\n",
    "    # with open(input_filename, 'r') as input_f:\n",
    "    #     for line in input_f:\n",
    "    #         # print(line)\n",
    "    #         json_obj = json.loads(line)\n",
    "    #         print(json_obj.get('question'))\n",
    "    #         print(json_obj.get('answer'))\n",
    "    #         print('######################')\n",
    "    #         print(encode_numbers_in_text(json_obj.get('question')))\n",
    "    #         print(encode_numbers_in_text(json_obj.get('answer')))\n",
    "    #         break\n",
    "    \n",
    "    N = 10\n",
    "    with open(input_filename, 'r') as input_f, open(output_filename, 'w') as output_f:\n",
    "        for i, line in enumerate(input_f, 1):\n",
    "            \n",
    "                \n",
    "            json_obj = json.loads(line)\n",
    "            \n",
    "            ground_truth_answer = json_obj['answer']\n",
    "            ground_truths.append(get_ground_truth_numerical_answer(ground_truth_answer))\n",
    "                                 \n",
    "            # prompt = generate_complete_user_prompt(json_obj['question'])\n",
    "            # print(prompt)\n",
    "            # payload['inputs'] = prompt\n",
    "            payload['inputs'] = format_few_shot_prompt(\"You are a diligent math student, logically solving the given math problems.\", x, z, y, json_obj['question'])\n",
    "            # print(payload['inputs'])\n",
    "            response = predictor.predict(payload)\n",
    "            # print(response)\n",
    "            # print(extract_answer_number(response[0]['generated_text']))\n",
    "            inference_answers.append(extract_answer_number(response[0]['generated_text']))\n",
    "\n",
    "            # print(ground_truths)\n",
    "            # print(inference_answers)\n",
    "            # print(\"#####\")\n",
    "\n",
    "            llama_text = response[0]\n",
    "            llama_text['prompt'] = payload['inputs']\n",
    "            json.dump(llama_text, output_f)\n",
    "            output_f.write('\\n')\n",
    "\n",
    "            if i % N == 0:\n",
    "                print(f\"Processed {i} lines...\")\n",
    "\n",
    "perform_gsm8k_inference('../test.jsonl', '../test_answers.jsonl')\n",
    "\n",
    "# print(ground_truths)\n",
    "# print(inference_answers)\n",
    "# print(\"#####\")\n",
    "\n",
    "accuracy = compute_accuracy(inference_answers, ground_truths)\n",
    "print(accuracy)\n",
    "with open('accuracy_results', 'w') as output_f:\n",
    "    output_f.write(str(accuracy))\n",
    "\n",
    "\n",
    "# generate_complete_user_prompt(\"Forty percent of the students have elected to learn from home during the pandemic. The remaining students are divided into two equal groups, only one of which is physically in school on any day. What percent of students are present in school?\")\n",
    "\n",
    "# perform_gsm8k_inference('test.jsonl', 'data/gsm8k/processed/test_processed.jsonl')\n",
    "\n",
    "# for i in range(1000):\n",
    "#     response = predictor.predict(payload)\n",
    "#     print(response)\n",
    "#     print(i)\n",
    "#     # print(extract_answer_number(response[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dee87945-9fd8-4aa9-b9ce-c992d48b59b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.0, 6.0, 75000.0, 180.0, 45.0, 5.08, 128.0, 16.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "print(inference_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c8b3f70a-7345-4c97-a814-935328d64c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.0, 3.0, 40000.0, 180.0, 20.0, 120.0, 20.0, 2.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "print(inference_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cbbef0-7af0-4e2b-a0b8-1d4c9476a860",
   "metadata": {},
   "source": [
    "# GSM8K RAG Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "09a15b1d-6c2b-4ad3-b62e-90f2be3cdf35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 lines...\n",
      "Processed 20 lines...\n",
      "Processed 30 lines...\n",
      "Processed 40 lines...\n",
      "Processed 50 lines...\n",
      "Processed 60 lines...\n",
      "Processed 70 lines...\n",
      "Processed 80 lines...\n",
      "Processed 90 lines...\n",
      "Processed 100 lines...\n",
      "Processed 110 lines...\n",
      "Processed 120 lines...\n",
      "Processed 130 lines...\n",
      "Processed 140 lines...\n",
      "Processed 150 lines...\n",
      "Processed 160 lines...\n",
      "Processed 170 lines...\n",
      "Processed 180 lines...\n",
      "Processed 190 lines...\n",
      "Processed 200 lines...\n",
      "Processed 210 lines...\n",
      "Processed 220 lines...\n",
      "Processed 230 lines...\n",
      "Processed 240 lines...\n",
      "Processed 250 lines...\n",
      "Processed 260 lines...\n",
      "Processed 270 lines...\n",
      "Processed 280 lines...\n",
      "Processed 290 lines...\n",
      "Processed 300 lines...\n",
      "Processed 310 lines...\n",
      "Processed 320 lines...\n",
      "Processed 330 lines...\n",
      "Processed 340 lines...\n",
      "Processed 350 lines...\n",
      "Processed 360 lines...\n",
      "Processed 370 lines...\n",
      "Processed 380 lines...\n",
      "Processed 390 lines...\n",
      "Processed 400 lines...\n",
      "Processed 410 lines...\n",
      "Processed 420 lines...\n",
      "Processed 430 lines...\n",
      "Processed 440 lines...\n",
      "Processed 450 lines...\n",
      "Processed 460 lines...\n",
      "Processed 470 lines...\n",
      "Processed 480 lines...\n",
      "Processed 490 lines...\n",
      "Processed 500 lines...\n",
      "Processed 510 lines...\n",
      "Processed 520 lines...\n",
      "Processed 530 lines...\n",
      "Processed 540 lines...\n",
      "Processed 550 lines...\n",
      "Processed 560 lines...\n",
      "Processed 570 lines...\n",
      "Processed 580 lines...\n",
      "Processed 590 lines...\n",
      "Processed 600 lines...\n",
      "Processed 610 lines...\n",
      "Processed 620 lines...\n",
      "Processed 630 lines...\n",
      "Processed 640 lines...\n",
      "Processed 650 lines...\n",
      "Processed 660 lines...\n",
      "Processed 670 lines...\n",
      "Processed 680 lines...\n",
      "Processed 690 lines...\n",
      "Processed 700 lines...\n",
      "Processed 710 lines...\n",
      "Processed 720 lines...\n",
      "Processed 730 lines...\n",
      "Processed 740 lines...\n",
      "Processed 750 lines...\n",
      "Processed 760 lines...\n",
      "Processed 770 lines...\n",
      "Processed 780 lines...\n",
      "Processed 790 lines...\n",
      "Processed 800 lines...\n",
      "Processed 810 lines...\n",
      "Processed 820 lines...\n",
      "Processed 830 lines...\n",
      "Processed 840 lines...\n",
      "Processed 850 lines...\n",
      "Processed 860 lines...\n",
      "Processed 870 lines...\n",
      "Processed 880 lines...\n",
      "Processed 890 lines...\n",
      "Processed 900 lines...\n",
      "Processed 910 lines...\n",
      "Processed 920 lines...\n",
      "Processed 930 lines...\n",
      "Processed 940 lines...\n",
      "Processed 950 lines...\n",
      "Processed 960 lines...\n",
      "Processed 970 lines...\n",
      "Processed 980 lines...\n",
      "Processed 990 lines...\n",
      "Processed 1000 lines...\n",
      "Processed 1010 lines...\n",
      "Processed 1020 lines...\n",
      "Processed 1030 lines...\n",
      "Processed 1040 lines...\n",
      "Processed 1050 lines...\n",
      "Processed 1060 lines...\n",
      "Processed 1070 lines...\n",
      "Processed 1080 lines...\n",
      "Processed 1090 lines...\n",
      "Processed 1100 lines...\n",
      "Processed 1110 lines...\n",
      "Processed 1120 lines...\n",
      "Processed 1130 lines...\n",
      "Processed 1140 lines...\n",
      "Processed 1150 lines...\n",
      "Processed 1160 lines...\n",
      "Processed 1170 lines...\n",
      "Processed 1180 lines...\n",
      "Processed 1190 lines...\n",
      "Processed 1200 lines...\n",
      "Processed 1210 lines...\n",
      "Processed 1220 lines...\n",
      "Processed 1230 lines...\n",
      "Processed 1240 lines...\n",
      "Processed 1250 lines...\n",
      "Processed 1260 lines...\n",
      "Processed 1270 lines...\n",
      "Processed 1280 lines...\n",
      "Processed 1290 lines...\n",
      "Processed 1300 lines...\n",
      "Processed 1310 lines...\n",
      "0.27520849128127367\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "ground_truths = []\n",
    "inference_answers = []\n",
    "\n",
    "def compute_accuracy(pred: list, gold: list):\n",
    "    acc = 0.0\n",
    "    for p, g in zip(pred, gold):\n",
    "        if p == g:\n",
    "            acc += 1\n",
    "\n",
    "    return acc / len(pred)\n",
    "\n",
    "def get_qa_pairs(input_str):\n",
    "    # Using regex to split the string into individual JSON strings\n",
    "    json_strings = re.findall(r'\\{.*?\\}', input_str)\n",
    "    \n",
    "    # Initializing lists to store questions and answers\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    # Parsing each JSON string and extracting questions and answers\n",
    "    for json_str in json_strings:\n",
    "        try:\n",
    "            json_obj = json.loads(json_str)\n",
    "            questions.append(json_obj['question'])\n",
    "            answers.append(json_obj['answer'])\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error decoding JSON from:\", json_str)\n",
    "    return questions, answers\n",
    "\n",
    "\n",
    "def format_few_shot_prompt(system_prompt, questions, answers, actual_question):\n",
    "    few_shot_prompt = \"\"\n",
    "    # \"inputs\": \"<s>[INST] <<SYS>>\\nYou are a diligent math student, logically solving given math problems\\n<</SYS>>\\n\\nIf you double a number and add 5 to the result, then that's 20 more than half of the original number. What's the original number? [/INST] Let x be the original number.\\n2*x+5=20+x/2\\n2*x-x/2=15\\n4*x-x=30\\n3*x=30\\nx=<<10=10>>10\\n#### 4</s><s>[INST] What is the units digit of the product of all the odd positive integers between 10 and 110? [/INST] \"\n",
    "    for i in range(len(questions)):\n",
    "        if i == 0:\n",
    "            few_shot_prompt += \"\\n\\n\" + questions[i] + \"\\n[/INST] \\n\" + answers[i] + \"\\n</s>\"\n",
    "        else:\n",
    "            few_shot_prompt += \"\\n<s>\\n[INST] \\n\" + questions[i] + \"\\n[/INST] \\n\" + answers[i] + \"\\n</s>\"\n",
    "        # break\n",
    "\n",
    "    few_shot_prompt += \"<s>\\n[INST] \\n\" + actual_question + \" \\n[/INST] \"\n",
    "    few_shot_prompt = \"<s>\\n[INST] \\n<<SYS>> \\n\" + system_prompt + \" \\n<</SYS>>\" + few_shot_prompt\n",
    "    return few_shot_prompt\n",
    "\n",
    "\n",
    "def get_ground_truth_numerical_answer(answer):\n",
    "    ans = answer.split('####')[-1]\n",
    "    ans = ans.replace(',', '')  # handle numbers like 2,000\n",
    "    try:\n",
    "        ans = float(ans)\n",
    "    except ValueError:\n",
    "        ans = float(\"inf\")\n",
    "    return ans\n",
    "\n",
    "\n",
    "def format_few_shot_prompt(system_prompt, questions, answers, actual_question):\n",
    "    few_shot_prompt = \"\"\n",
    "    # \"inputs\": \"<s>[INST] <<SYS>>\\nYou are a diligent math student, logically solving given math problems\\n<</SYS>>\\n\\nIf you double a number and add 5 to the result, then that's 20 more than half of the original number. What's the original number? [/INST] Let x be the original number.\\n2*x+5=20+x/2\\n2*x-x/2=15\\n4*x-x=30\\n3*x=30\\nx=<<10=10>>10\\n#### 4</s><s>[INST] What is the units digit of the product of all the odd positive integers between 10 and 110? [/INST] \"\n",
    "    for i in range(len(questions)):\n",
    "        if i == 0:\n",
    "            few_shot_prompt += \"\\n\\n\" + questions[i] + \"\\n[/INST] \\n\" + answers[i] + \"\\n</s>\"\n",
    "        else:\n",
    "            few_shot_prompt += \"\\n<s>\\n[INST] \\n\" + questions[i] + \"\\n[/INST] \\n\" + answers[i] + \"\\n</s>\"\n",
    "        # break\n",
    "\n",
    "    few_shot_prompt += \"<s>\\n[INST] \\n\" + actual_question + \" \\n[/INST] \"\n",
    "    few_shot_prompt = \"<s>\\n[INST] \\n<<SYS>> \\n\" + system_prompt + \" \\n<</SYS>>\" + few_shot_prompt\n",
    "    return few_shot_prompt\n",
    "\n",
    "\n",
    "N = 10\n",
    "with open(\"../gsm8k_rag.jsonl\", 'r') as input_f, open(\"../test_answer_gsmNormal_rag.jsonl\", 'w') as output_f:\n",
    "    for i, line in enumerate(input_f, 1):\n",
    "\n",
    "        json_obj = json.loads(line)\n",
    "            \n",
    "        ground_truth_answer = json_obj['answer']\n",
    "        ground_truths.append(get_ground_truth_numerical_answer(ground_truth_answer))\n",
    "\n",
    "        questions,answers = get_qa_pairs(json_obj['prompt'])\n",
    "        input_prompt = format_few_shot_prompt(\"You are a diligent math student, logically solving the given math problems.\", questions, answers, json_obj['question'])\n",
    "        # print(input_prompt)\n",
    "\n",
    "        payload = {\n",
    "            \"inputs\": input_prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 512,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": 0.6\n",
    "            }\n",
    "        }\n",
    "\n",
    "        response = predictor.predict(payload)\n",
    "        # print(response)\n",
    "        # break\n",
    "\n",
    "        inference_answers.append(extract_answer_number(response[0]['generated_text']))\n",
    "\n",
    "        # print(ground_truths)\n",
    "        # print(inference_answers)\n",
    "        # print(\"#####\")\n",
    "\n",
    "        llama_text = response[0]\n",
    "        llama_text['prompt'] = payload['inputs']\n",
    "        json.dump(llama_text, output_f)\n",
    "        output_f.write('\\n')\n",
    "\n",
    "        if i % N == 0:\n",
    "            print(f\"Processed {i} lines...\")\n",
    "\n",
    "\n",
    "accuracy = compute_accuracy(inference_answers, ground_truths)\n",
    "print(accuracy)\n",
    "with open('accuracy_results_gsmNormal.txt', 'w') as output_f:\n",
    "    output_f.write(str(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f2e87-4a5d-45c7-9197-bc52e7ece973",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MATH prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bc62ca8a-0ecd-4adf-bc85-c1d55636c15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'To find the probability that it will not rain tomorrow, we can subtract the probability of rain from 1:\\n\\nProbability of not raining = 1 - Probability of raining\\n= 1 - (3/10)\\n= 7/10\\n\\nSo, the probability that it will not rain tomorrow is 7/10.\\n\\nNote: Since the probability of rain is given as a fraction, we can simplify it by multiplying both the numerator and denominator by 10 to make the fraction easier to work with:\\n\\nProbability of raining = 3/10\\nProbability of not raining = 7/10\\n\\nTherefore, the probability that it will not rain tomorrow is 7/10.'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def get_qa_pairs(input_str):\n",
    "    # Using regex to split the string into individual JSON strings\n",
    "    json_strings = re.findall(r'\\{.*?\\}', input_str)\n",
    "    \n",
    "    # Initializing lists to store questions and answers\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    # Parsing each JSON string and extracting questions and answers\n",
    "    for json_str in json_strings:\n",
    "        try:\n",
    "            json_obj = json.loads(json_str)\n",
    "            questions.append(json_obj['question'])\n",
    "            answers.append(json_obj['answer'])\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error decoding JSON from:\", json_str)\n",
    "    return questions, answers\n",
    "\n",
    "def format_few_shot_prompt(system_prompt, questions, answers, actual_question):\n",
    "    few_shot_prompt = \"\"\n",
    "    # \"inputs\": \"<s>[INST] <<SYS>>\\nYou are a diligent math student, logically solving given math problems\\n<</SYS>>\\n\\nIf you double a number and add 5 to the result, then that's 20 more than half of the original number. What's the original number? [/INST] Let x be the original number.\\n2*x+5=20+x/2\\n2*x-x/2=15\\n4*x-x=30\\n3*x=30\\nx=<<10=10>>10\\n#### 4</s><s>[INST] What is the units digit of the product of all the odd positive integers between 10 and 110? [/INST] \"\n",
    "    for i in range(len(questions)):\n",
    "        if i == 0:\n",
    "            few_shot_prompt += \"\\n\\n\" + questions[i] + \"\\n[/INST] \\n\" + answers[i] + \"\\n</s>\"\n",
    "        else:\n",
    "            few_shot_prompt += \"\\n<s>\\n[INST] \\n\" + questions[i] + \"\\n[/INST] \\n\" + answers[i] + \"\\n</s>\"\n",
    "        # break\n",
    "\n",
    "    few_shot_prompt += \"<s>\\n[INST] \\n\" + actual_question + \" \\n[/INST] \"\n",
    "    few_shot_prompt = \"<s>\\n[INST] \\n<<SYS>> \\n\" + system_prompt + \" \\n<</SYS>>\" + few_shot_prompt\n",
    "    return few_shot_prompt\n",
    "                \n",
    "    \n",
    "\n",
    "with open(\"../math_test_set/counting_and_probability/82.json\", 'r') as input_f:\n",
    "    json_obj = json.load(input_f)\n",
    "\n",
    "\n",
    "\n",
    "questions,answers = get_qa_pairs(json_obj['prompt'])\n",
    "# print(len(questions))\n",
    "# print(len(answers))\n",
    "input_prompt = format_few_shot_prompt(\"You are a diligent math student, logically solving the given math problems.\", questions, answers, json_obj['problem'])\n",
    "# print(input_prompt)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": input_prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "response = predictor.predict(payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69483411-5f98-4bc5-94c3-831d46c7d724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/DemoNotebooks'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc277a2",
   "metadata": {},
   "source": [
    "This model supports the following payload parameters. You may specify any subset of these parameters when invoking an endpoint.\n",
    "\n",
    "* **do_sample:** If True, activates logits sampling. If specified, it must be boolean.\n",
    "* **max_new_tokens:** Maximum number of generated tokens. If specified, it must be a positive integer.\n",
    "* **repetition_penalty:** A penalty for repetitive generated text. 1.0 means no penalty.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "* **stop**: If specified, it must a list of strings. Text generation stops if any one of the specified strings is generated.\n",
    "* **seed**: Random sampling seed.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **truncate:** Truncate inputs tokens to the given size.\n",
    "* **typical_p:** Typical decoding mass, according to [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666).\n",
    "* **best_of:** Generate best_of sequences and return the one if the highest token logprobs.\n",
    "* **watermark:** Whether to perform watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226).\n",
    "* **details:** Return generation details, to include output token logprobs and IDs.\n",
    "* **decoder_input_details:** Return decoder input token logprobs and IDs.\n",
    "* **top_n_tokens:** Return the N most likely tokens at each step."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
